{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shakespeare Sonnets with LSTMs\n",
    "\n",
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded a corpus of 94652 characters\n",
      "\n",
      "Data sample:\n",
      "\n",
      "ï»¿From fairest creatures we desire increase,\n",
      "That thereby beauty's rose might never die,\n",
      "But as the riper should by time decease,\n",
      "His tender heir might bear his memory:\n",
      "But thou, contracted to thine ow\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Activation\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "\n",
    "with open(\"sonnets.txt\") as corpus_file:\n",
    "    corpus = corpus_file.read()\n",
    "print(\"Loaded a corpus of {0} characters\".format(len(corpus)))\n",
    "\n",
    "print('\\nData sample:\\n')\n",
    "print(corpus[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our corpus contains 62 unique characters.\n"
     ]
    }
   ],
   "source": [
    "# Get a unique identifier for each char in the corpus, then make some dicts to ease encoding and decoding\n",
    "chars = sorted(list(set(corpus)))\n",
    "num_chars = len(chars)\n",
    "encoding = {c: i for i, c in enumerate(chars)}\n",
    "decoding = {i: c for i, c in enumerate(chars)}\n",
    "print(\"Our corpus contains {0} unique characters.\".format(num_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sliced our corpus into 94602 sentences of length 50\n"
     ]
    }
   ],
   "source": [
    "# chop up our data into X and y, slice into roughly (num_chars / skip) overlapping 'sentences'\n",
    "# of length sentence_length, and encode the chars\n",
    "sentence_length = 50\n",
    "skip = 1\n",
    "X_data = []\n",
    "y_data = []\n",
    "for i in range (0, len(corpus) - sentence_length, skip):\n",
    "    sentence = corpus[i:i + sentence_length]\n",
    "    next_char = corpus[i + sentence_length]\n",
    "    X_data.append([encoding[char] for char in sentence])\n",
    "    y_data.append(encoding[next_char])\n",
    "\n",
    "num_sentences = len(X_data)\n",
    "print(\"Sliced our corpus into {0} sentences of length {1}\".format(num_sentences, sentence_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing X and y...\n",
      "Sanity check y. Dimension: (94602, 62) # Sentences: 94602 Characters in corpus: 62\n",
      "Sanity check X. Dimension: (94602, 50, 62) Sentence length: 50\n"
     ]
    }
   ],
   "source": [
    "# Vectorize our data and labels. We want everything in one-hot\n",
    "print(\"Vectorizing X and y...\")\n",
    "X = np.zeros((num_sentences, sentence_length, num_chars), dtype=np.bool)\n",
    "y = np.zeros((num_sentences, num_chars), dtype=np.bool)\n",
    "for i, sentence in enumerate(X_data):\n",
    "    for t, encoded_char in enumerate(sentence):\n",
    "        X[i, t, encoded_char] = 1\n",
    "    y[i, y_data[i]] = 1\n",
    "\n",
    "# Double check our vectorized data before we sink hours into fitting a model\n",
    "print(\"Sanity check y. Dimension: {0} # Sentences: {1} Characters in corpus: {2}\".format(y.shape, num_sentences, len(chars)))\n",
    "print(\"Sanity check X. Dimension: {0} Sentence length: {1}\".format(X.shape, sentence_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 256)               326656    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 62)                15934     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 62)                0         \n",
      "=================================================================\n",
      "Total params: 342,590\n",
      "Trainable params: 342,590\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(sentence_length, num_chars)))\n",
    "model.add(Dense(num_chars))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture = model.to_yaml()\n",
    "with open('model.yaml', 'a') as model_file:\n",
    "    model_file.write(architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=\"weights-{epoch:02d}-{loss:.3f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor=\"loss\", verbose=1, save_best_only=True, mode=\"min\")\n",
    "tensorboard = TensorBoard(log_dir='./logs/min_train_loss')\n",
    "callbacks = [checkpoint, tensorboard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "94602/94602 [==============================] - 43s 451us/step - loss: 2.2297\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.22969, saving model to weights-01-2.230.hdf5\n",
      "Epoch 2/30\n",
      "94602/94602 [==============================] - 44s 470us/step - loss: 2.0087\n",
      "\n",
      "Epoch 00002: loss improved from 2.22969 to 2.00871, saving model to weights-02-2.009.hdf5\n",
      "Epoch 3/30\n",
      "94602/94602 [==============================] - 45s 477us/step - loss: 1.8958\n",
      "\n",
      "Epoch 00003: loss improved from 2.00871 to 1.89584, saving model to weights-03-1.896.hdf5\n",
      "Epoch 4/30\n",
      "94602/94602 [==============================] - 45s 473us/step - loss: 1.8066\n",
      "\n",
      "Epoch 00004: loss improved from 1.89584 to 1.80656, saving model to weights-04-1.807.hdf5\n",
      "Epoch 5/30\n",
      "94602/94602 [==============================] - 41s 431us/step - loss: 1.7398\n",
      "\n",
      "Epoch 00005: loss improved from 1.80656 to 1.73977, saving model to weights-05-1.740.hdf5\n",
      "Epoch 6/30\n",
      "94602/94602 [==============================] - 38s 401us/step - loss: 1.6821\n",
      "\n",
      "Epoch 00006: loss improved from 1.73977 to 1.68214, saving model to weights-06-1.682.hdf5\n",
      "Epoch 7/30\n",
      "94602/94602 [==============================] - 40s 421us/step - loss: 1.6291\n",
      "\n",
      "Epoch 00007: loss improved from 1.68214 to 1.62910, saving model to weights-07-1.629.hdf5\n",
      "Epoch 8/30\n",
      "94602/94602 [==============================] - 43s 458us/step - loss: 1.5791\n",
      "\n",
      "Epoch 00008: loss improved from 1.62910 to 1.57913, saving model to weights-08-1.579.hdf5\n",
      "Epoch 9/30\n",
      "94602/94602 [==============================] - 47s 500us/step - loss: 1.5341\n",
      "\n",
      "Epoch 00009: loss improved from 1.57913 to 1.53409, saving model to weights-09-1.534.hdf5\n",
      "Epoch 10/30\n",
      "94602/94602 [==============================] - 48s 509us/step - loss: 1.4883\n",
      "\n",
      "Epoch 00010: loss improved from 1.53409 to 1.48826, saving model to weights-10-1.488.hdf5\n",
      "Epoch 11/30\n",
      "94602/94602 [==============================] - 49s 513us/step - loss: 1.4454\n",
      "\n",
      "Epoch 00011: loss improved from 1.48826 to 1.44539, saving model to weights-11-1.445.hdf5\n",
      "Epoch 12/30\n",
      "94602/94602 [==============================] - 49s 515us/step - loss: 1.4044\n",
      "\n",
      "Epoch 00012: loss improved from 1.44539 to 1.40436, saving model to weights-12-1.404.hdf5\n",
      "Epoch 13/30\n",
      "94602/94602 [==============================] - 49s 514us/step - loss: 1.3613\n",
      "\n",
      "Epoch 00013: loss improved from 1.40436 to 1.36133, saving model to weights-13-1.361.hdf5\n",
      "Epoch 14/30\n",
      "94602/94602 [==============================] - 49s 518us/step - loss: 1.3191\n",
      "\n",
      "Epoch 00014: loss improved from 1.36133 to 1.31911, saving model to weights-14-1.319.hdf5\n",
      "Epoch 15/30\n",
      "94602/94602 [==============================] - 49s 519us/step - loss: 1.2761\n",
      "\n",
      "Epoch 00015: loss improved from 1.31911 to 1.27606, saving model to weights-15-1.276.hdf5\n",
      "Epoch 16/30\n",
      "94602/94602 [==============================] - 44s 462us/step - loss: 1.2338\n",
      "\n",
      "Epoch 00016: loss improved from 1.27606 to 1.23382, saving model to weights-16-1.234.hdf5\n",
      "Epoch 17/30\n",
      "94602/94602 [==============================] - 45s 475us/step - loss: 1.1895\n",
      "\n",
      "Epoch 00017: loss improved from 1.23382 to 1.18946, saving model to weights-17-1.189.hdf5\n",
      "Epoch 18/30\n",
      "94602/94602 [==============================] - 46s 484us/step - loss: 1.1457\n",
      "\n",
      "Epoch 00018: loss improved from 1.18946 to 1.14574, saving model to weights-18-1.146.hdf5\n",
      "Epoch 19/30\n",
      "94602/94602 [==============================] - 45s 478us/step - loss: 1.1032\n",
      "\n",
      "Epoch 00019: loss improved from 1.14574 to 1.10318, saving model to weights-19-1.103.hdf5\n",
      "Epoch 20/30\n",
      "94602/94602 [==============================] - 45s 477us/step - loss: 1.0631\n",
      "\n",
      "Epoch 00020: loss improved from 1.10318 to 1.06311, saving model to weights-20-1.063.hdf5\n",
      "Epoch 21/30\n",
      "94602/94602 [==============================] - 47s 498us/step - loss: 1.0208\n",
      "\n",
      "Epoch 00021: loss improved from 1.06311 to 1.02076, saving model to weights-21-1.021.hdf5\n",
      "Epoch 22/30\n",
      "94602/94602 [==============================] - 49s 516us/step - loss: 0.9808\n",
      "\n",
      "Epoch 00022: loss improved from 1.02076 to 0.98078, saving model to weights-22-0.981.hdf5\n",
      "Epoch 23/30\n",
      "94602/94602 [==============================] - 49s 516us/step - loss: 0.9715\n",
      "\n",
      "Epoch 00023: loss improved from 0.98078 to 0.97145, saving model to weights-23-0.971.hdf5\n",
      "Epoch 24/30\n",
      "94602/94602 [==============================] - 50s 523us/step - loss: 0.9197\n",
      "\n",
      "Epoch 00024: loss improved from 0.97145 to 0.91967, saving model to weights-24-0.920.hdf5\n",
      "Epoch 25/30\n",
      "94602/94602 [==============================] - 50s 532us/step - loss: 0.9003\n",
      "\n",
      "Epoch 00025: loss improved from 0.91967 to 0.90034, saving model to weights-25-0.900.hdf5\n",
      "Epoch 26/30\n",
      "94602/94602 [==============================] - 51s 535us/step - loss: 0.8353\n",
      "\n",
      "Epoch 00026: loss improved from 0.90034 to 0.83535, saving model to weights-26-0.835.hdf5\n",
      "Epoch 27/30\n",
      "94602/94602 [==============================] - 51s 544us/step - loss: 0.8064\n",
      "\n",
      "Epoch 00027: loss improved from 0.83535 to 0.80639, saving model to weights-27-0.806.hdf5\n",
      "Epoch 28/30\n",
      "94602/94602 [==============================] - 48s 508us/step - loss: 0.7718\n",
      "\n",
      "Epoch 00028: loss improved from 0.80639 to 0.77182, saving model to weights-28-0.772.hdf5\n",
      "Epoch 29/30\n",
      "94602/94602 [==============================] - 45s 478us/step - loss: 0.7402\n",
      "\n",
      "Epoch 00029: loss improved from 0.77182 to 0.74021, saving model to weights-29-0.740.hdf5\n",
      "Epoch 30/30\n",
      "94602/94602 [==============================] - 40s 425us/step - loss: 0.7134\n",
      "\n",
      "Epoch 00030: loss improved from 0.74021 to 0.71345, saving model to weights-30-0.713.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f98040b7f28>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=30, batch_size=128, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_yaml\n",
    "\n",
    "with open(\"model.yaml\") as model_file:\n",
    "    architecture = model_file.read()\n",
    "\n",
    "model = model_from_yaml(architecture)\n",
    "model.load_weights(\"weights-30-0.713.hdf5\")\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start the generating with the seed phrase:\n",
      "\n",
      " thy beauty's use,\n",
      "If thou couldst answer 'This fa\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "corpus_length = len(corpus)\n",
    "#seed = randint(0, corpus_length - sentence_length)\n",
    "seed = 1000\n",
    "seed_phrase = corpus[seed:seed + sentence_length]\n",
    "print('Start the generating with the seed phrase:\\n')\n",
    "print(seed_phrase)\n",
    "\n",
    "X = np.zeros((1, sentence_length, num_chars), dtype=np.bool)\n",
    "for i, character in enumerate(seed_phrase):\n",
    "    X[0, i, encoding[character]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Input characters:\n",
      "\n",
      " thy beauty's use,\n",
      "If thou couldst answer 'This fa\n",
      "\n",
      " Predicted character: i\n",
      "\n",
      " Input characters:\n",
      "\n",
      "thy beauty's use,\n",
      "If thou couldst answer 'This fai\n",
      "\n",
      " Predicted character: r\n",
      "\n",
      " Input characters:\n",
      "\n",
      "hy beauty's use,\n",
      "If thou couldst answer 'This fair\n",
      "\n",
      " Predicted character:  \n",
      "\n",
      " Input characters:\n",
      "\n",
      "y beauty's use,\n",
      "If thou couldst answer 'This fair \n",
      "\n",
      " Predicted character: h\n",
      "\n",
      " Input characters:\n",
      "\n",
      " beauty's use,\n",
      "If thou couldst answer 'This fair h\n",
      "\n",
      " Predicted character: o\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ada0df7c8b734d2397d25bb84bdec57a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=450), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Final generated text:\n",
      "\n",
      "ir holds her prime,\n",
      "Which hath heavy that make me with sweet soull\n",
      "Of that which thou dost best earth than truth still,\n",
      "And summer's lovion of your every pleasure,\n",
      "When I more the summer's dost in his grown,\n",
      "And thou art all myself doth stard and loven,\n",
      "As shadow in your sweet state words he call\n",
      "  And thou to have I hos excuse the state,\n",
      "And thou art butted that the learnot comment\n",
      "Dear heaven for myself with thee shall fave,\n",
      "That do to me to the wor\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# small loop illustrating the generation process\n",
    "generated_text = \"\"\n",
    "for i in range(5):\n",
    "    print('\\n Input characters:\\n')\n",
    "    print(\"\".join([decoding[i] for i in np.argmax(X[0,:,:],axis=1)]))\n",
    "    \n",
    "    prediction = np.argmax(model.predict(X, verbose=0))\n",
    "    \n",
    "    print(f'\\n Predicted character: {decoding[prediction]}')\n",
    "\n",
    "    generated_text += decoding[prediction]\n",
    "\n",
    "    activations = np.zeros((1, 1, num_chars), dtype=np.bool)\n",
    "    activations[0, 0, prediction] = 1\n",
    "    X = np.concatenate((X[:, 1:, :], activations), axis=1)\n",
    "    \n",
    "pbar = tqdm(range(450))\n",
    "for i in range(450):\n",
    "    prediction = np.argmax(model.predict(X, verbose=0))\n",
    "\n",
    "    generated_text += decoding[prediction]\n",
    "\n",
    "    activations = np.zeros((1, 1, num_chars), dtype=np.bool)\n",
    "    activations[0, 0, prediction] = 1\n",
    "    X = np.concatenate((X[:, 1:, :], activations), axis=1)\n",
    "    pbar.update()\n",
    "pbar.close()\n",
    "\n",
    "print('\\n Final generated text:\\n')\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** In the above example, we've generated text by making predictions and taking the character with the *maximum activation* at each point. In practice, more creative and interesting results can often be found by introducing some *randomness* into the generation. \n",
    "\n",
    "Instead of the `argmax` rule for selecting the predicted character at each point, experiment with introducing randomness into the generation process (for instance, choosing the predicted character with probability proportional to its softmax activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
